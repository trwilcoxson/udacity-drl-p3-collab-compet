{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "This notebook trains a self-play DDPG agent to solve the Unity Tennis environment, where two agents learn to keep a ball in play over a net.\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from maddpg_agent import Agent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='Tennis.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Two agents control rackets to bounce a ball over a net:\n",
    "- **Reward**: +0.1 for hitting the ball over the net, -0.01 if ball hits ground or goes out of bounds\n",
    "- **Observation**: 24 dimensions per agent (3 stacked frames × 8 variables: ball/racket position and velocity)\n",
    "- **Action**: 2 continuous values per agent (movement toward/away from net, jumping), each in [-1, 1]\n",
    "- **Solve condition**: Average of max(agent1_score, agent2_score) >= 0.5 over 100 consecutive episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print(f'Number of agents: {num_agents}')\n",
    "\n",
    "# Size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print(f'Size of each action: {action_size}')\n",
    "\n",
    "# Examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print(f'There are {num_agents} agents. Each observes a state with length: {state_size}')\n",
    "print(f'The state for the first agent looks like:\\n{states[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent\n",
    "\n",
    "We use **self-play DDPG**: both agents share one actor, one critic, and one replay buffer. This works because Tennis is symmetric — an optimal policy for one side is optimal for the other.\n",
    "\n",
    "Key adaptation from the Continuous Control project: **noise decay** (σ × 0.9995 per episode, with floor 0.01). In sparse-reward environments, the agent needs heavy exploration early but must quiet down once it finds a fragile cooperative equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size,\n",
    "              num_agents=num_agents, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_episodes = 5000\n",
    "max_t = 1000\n",
    "all_scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "solved = False\n",
    "t_start = time.time()\n",
    "\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    agent.reset()\n",
    "    scores = np.zeros(num_agents)\n",
    "\n",
    "    for t in range(max_t):\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        agent.step(states, actions, rewards, next_states, dones)\n",
    "        states = next_states\n",
    "        scores += rewards\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "    # Tennis scoring: max over the two agents\n",
    "    episode_score = np.max(scores)\n",
    "    all_scores.append(episode_score)\n",
    "    scores_window.append(episode_score)\n",
    "    rolling_avg = np.mean(scores_window)\n",
    "\n",
    "    # Decay exploration noise\n",
    "    agent.decay_noise()\n",
    "\n",
    "    if i_episode % 100 == 0:\n",
    "        elapsed = time.time() - t_start\n",
    "        print(f'Episode {i_episode}\\tAvg: {rolling_avg:.3f}\\tNoise σ: {agent.noise_sigma:.4f}\\t({elapsed:.0f}s)')\n",
    "\n",
    "    if rolling_avg >= 0.5 and not solved:\n",
    "        elapsed = time.time() - t_start\n",
    "        print(f'\\n*** Solved at episode {i_episode}!  100-ep avg: {rolling_avg:.3f} ({elapsed:.0f}s) ***')\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        solved = True\n",
    "        break  # Stop training once solved\n",
    "\n",
    "if not solved:\n",
    "    print(f'\\nDid not solve. Final 100-ep avg: {np.mean(scores_window):.3f}')\n",
    "    torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "    torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "\n",
    "np.save('scores.npy', np.array(all_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot Training Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load('scores.npy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(np.arange(1, len(scores) + 1), scores, alpha=0.3, color='steelblue', label='Episode Score')\n",
    "if len(scores) >= 100:\n",
    "    rolling = [np.mean(scores[max(0, i-100):i]) for i in range(1, len(scores) + 1)]\n",
    "    ax.plot(np.arange(1, len(scores) + 1), rolling, color='darkblue', linewidth=2, label='100-Episode Average')\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Solve Threshold (0.5)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Score (Max over Agents)')\n",
    "ax.set_title('Self-Play DDPG Training — Tennis')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('scores_plot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved scores_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Greedy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', weights_only=True))\n",
    "\n",
    "test_scores = []\n",
    "for i in range(1, 101):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    ep_scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "        actions = agent.act(states, add_noise=False)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        ep_scores += env_info.rewards\n",
    "        if np.any(env_info.local_done):\n",
    "            break\n",
    "    test_scores.append(np.max(ep_scores))\n",
    "    if i % 20 == 0:\n",
    "        print(f'Test {i}/100  Score: {np.max(ep_scores):.2f}')\n",
    "\n",
    "print(f'\\nGreedy Test Results (100 episodes):')\n",
    "print(f'  Average: {np.mean(test_scores):.3f}')\n",
    "print(f'  Std Dev: {np.std(test_scores):.3f}')\n",
    "print(f'  Min:     {np.min(test_scores):.3f}')\n",
    "print(f'  Max:     {np.max(test_scores):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (drlnd-nav)",
   "language": "python",
   "name": "drlnd-nav"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
